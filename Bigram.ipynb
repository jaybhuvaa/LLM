{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiPq1e9rg8qynoky619kcS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaybhuvaa/LLM/blob/main/Bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbKgYUaStObO",
        "outputId": "eb7744b4-31d6-4dba-cdcd-d754ca4e57c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232131\n",
            "ï»¿  DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW\n"
          ]
        }
      ],
      "source": [
        "with open('woo.txt','r',encoding='utf-8') as f:\n",
        "    text=f.read()\n",
        "print(len(text))\n",
        "print(text[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**\n",
        "\n",
        "Tokenization in Python is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks."
      ],
      "metadata": {
        "id": "yrIXR8T5vEZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars=sorted(set(text))\n",
        "print(chars)\n",
        "print(len(chars))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9051iX1tvQV",
        "outputId": "69c9027d-d624-4c2b-e1c4-01e57567ee3a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding-Deconding**\n",
        "\n",
        "string_to_int = {ch:i for i , ch in enumerate (chars)}:\n",
        "\n",
        "This line creates a dictionary string_to_int where each character in the chars string is mapped to its corresponding index in the chars string. The enumerate() function provides both the index and the character from the chars string.\n",
        "\n",
        "encode=lambda s:[string_to_int[c] for c in s]:\n",
        "\n",
        "This line defines an anonymous function encode which takes a string s as input. It uses a list comprehension to iterate over each character c in the input string s, and for each character, it retrieves its corresponding integer value from the string_to_int dictionary and forms a list of these integer values."
      ],
      "metadata": {
        "id": "4Ddb_2GduqUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {ch:i for i , ch in enumerate (chars)}\n",
        "int_to_string = {i:ch for i , ch in enumerate (chars)}\n",
        "encode=lambda s:[string_to_int[c] for c in s]\n",
        "decode=lambda l:''.join([int_to_string[i] for i in l])\n",
        "\n",
        "print(encode('Hello'))\n",
        "print(encode('Jay'))\n",
        "print(decode([32, 58, 65]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aiHH7CYtyG4",
        "outputId": "a00551f0-020c-4e1c-dca3-28bd408d45dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 58, 65, 65, 68]\n",
            "[34, 54, 78]\n",
            "Hel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "message2 = \"A fast brown fox leaps over a sleepy dog.\"\n",
        "\n",
        "emsg1 = encode(message1)\n",
        "emsg2 = encode(message2)\n",
        "\n",
        "print(type(emsg1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExchVBNTt-TC",
        "outputId": "4f8cdc97-34cd-481a-9d66-f352c5e2cee8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch**\n",
        "\n",
        "PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library. Torch is an open source ML library used for creating deep neural networks and is written in the Lua scripting language. It's one of the preferred platforms for deep learning research. The framework is built to speed up the process between research prototyping and deployment."
      ],
      "metadata": {
        "id": "8N9H1ZrCuPwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data=torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMDjwQu4t1X0",
        "outputId": "5ac52bf3-5a22-42ca-f6db-85b4c6b74fa3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
            "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
            "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
            "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
            "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
            "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test-Validation Split**\n",
        "\n",
        "> Why to split data?\n",
        "\n",
        "  if we give model our full data to train than there is no any\n",
        "    prediction part. it will be only factual data that will be given by model.\n",
        "    \n",
        " if we split the data than model will be train based on the taining data anf to test the accuracy we can use the test data to check weather the model is doing right or wrong.\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "561TdsUQwjM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.8*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]\n",
        "# print(train_data[:10])\n",
        "# print(val_data[:10])\n",
        "\n",
        "def get_batch(split):\n",
        "    data=train_data if split=='train' else val_data\n",
        "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "    print(ix)\n",
        "    x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x,y\n",
        "\n",
        "xb,yb=get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUfMxS7auOMw",
        "outputId": "2b628e18-68b5-4cd1-adfa-a8e44a0fe977"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([161468,  40843, 134403, 125247])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[73,  1, 61, 58, 71,  1, 62, 67],\n",
            "        [ 9,  3,  1, 72, 54, 62, 57,  1],\n",
            "        [68, 76,  1, 76, 54, 72,  1, 65],\n",
            "        [72, 73, 71, 58, 73, 56, 61, 58]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 61, 58, 71,  1, 62, 67,  1],\n",
            "        [ 3,  1, 72, 54, 62, 57,  1, 73],\n",
            "        [76,  1, 76, 54, 72,  1, 65, 62],\n",
            "        [73, 71, 58, 73, 56, 61, 58, 57]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigram Model**\n",
        "\n",
        "A bigram language model is a type of statistical language model that predicts the probability of a word in a sequence based on the previous word. It considers pairs of consecutive words (bigrams) and estimates the likelihood of encountering a specific word given the preceding word in a text or sentence."
      ],
      "metadata": {
        "id": "nJPiyXIUzL1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=8\n",
        "batch_size=4\n",
        "\n",
        "x=train_data[:block_size+1]\n",
        "y=train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context=x[:t+1]\n",
        "    target=y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2FY6mXNyyt8",
        "outputId": "8378ed3e-c7e8-4e91-a529-b298f8f5ca36"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([80]) the target: 1\n",
            "when input is tensor([80,  1]) the target: 1\n",
            "when input is tensor([80,  1,  1]) the target: 28\n",
            "when input is tensor([80,  1,  1, 28]) the target: 39\n",
            "when input is tensor([80,  1,  1, 28, 39]) the target: 42\n",
            "when input is tensor([80,  1,  1, 28, 39, 42]) the target: 39\n",
            "when input is tensor([80,  1,  1, 28, 39, 42, 39]) the target: 44\n",
            "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) the target: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuda is used for gpu based computation or training purpose."
      ],
      "metadata": {
        "id": "-4EboCM-0gf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOHa9d1i0B4M",
        "outputId": "85727b54-668e-403e-9aa9-cc56562bc912"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        logits = self.token_embedding_table(index)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(index)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = BigramLanguageModel(len(chars))\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "HqAcEOq20obK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07bde88-7f6d-4791-a124-47910b4f79cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uï»¿Tr.WcnXR:02s863kzC]3'h JnwW\"fPgN6eï»¿Hg\n",
            "34Tq-4ZhRN&fu7\n",
            "aqaï»¿(([,p!,S?lO?)6e?*eU(i\n",
            "kFh-XWF0Uk0H j,o)T3'Mj-4&XHvNv_BBovYPcRi8eEE-z\n",
            "kl&0,Qd&\n",
            "r_49)MFexSWx.B:V&N.Ezp;gjtR2:aaQ OCaajRa!F_-sa5hb6Bfï»¿(dbZr)6aLKGVLR:0myf!Dq:WoT?tv7Upu u;(icKg?*99FZSDl)\"iU!r'ï»¿s]n:0v\n",
            ")lfH9*,GMkis(KCyuW4L'&8.rco)Jkz29ynOEDMUJlX9Ks y!ï»¿G3Pghv!mpuXAfCIz(anXZ(d\"FEza5zDVD:MFE3vBMzH1H;x 9v0uX\"P)6xKJmOR[,(4SKGS\"Q )jD[YOuDl2psLAGMZ,U!q_K6JybYWK7Z!V(vGUA0YKcwd6Jl\"7OCS\"Qyo,4qZNSmL:3,6e?*?myTVJ:0xp;!-Boo?v'W8yï»¿7wiUgBoz2HX\"9xVRaFG2R6po4]\n"
          ]
        }
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "9rIWoPVTsdWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}